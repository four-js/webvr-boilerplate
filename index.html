<!DOCTYPE html>

<html lang="en">
<head>
<title>Web VR boilerplate (Cardboard and Oculus)</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
<meta name="mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<style>
body {
  width: 100%;
  height: 100%;
  position: fixed;
  background-color: #000;
  color: #fff;
  margin: 0px;
  padding: 0;
  overflow: hidden;
}
</style>
</head>

<body>

</body>

<!--
  three.js 3d library
  -->
<script src="js/deps/three.js"></script>

<!--
  VRControls.js acquires positional information from connected VR devices and applies the transformations to a three.js camera object.
   -->
<script src="js/deps/VRControls.js"></script>

<!--
  VREffect.js handles stereo camera setup and rendering.
  -->
<script src="js/deps/VREffect.js"></script>

<!--
  A polyfill for WebVR using the Device{Motion,Orientation}Event API.
  -->
<script src="js/deps/webvr-polyfill.js"></script>

<!--
  Helps enter and exit VR mode, provides best practices while in VR.
  -->
<script src="build/webvr-manager.js"></script>

<script>
/*
 * Debug parameters.
 */

// Enable distortion everywhere.
//WEBVR_FORCE_DISTORTION = true;
// Override the distortion background color.
//WEBVR_BACKGROUND_COLOR = new THREE.Vector4(1, 0, 0, 1);
// Change the tracking prediction mode.
//WEBVR_PREDICTION_MODE = 2;
// In prediction mode, change how far into the future to predict.
//WEBVR_PREDICTION_TIME_MS = 100;
</script>

<script id="vs" type="x-shader/x-vertex">

  uniform sampler2D map;

  uniform float width;
  uniform float height;
  uniform float nearClipping, farClipping;

  uniform float pointSize;
  uniform float zOffset;

  varying vec2 vUv;

  const float XtoZ = 1.11146; // tan( 1.0144686 / 2.0 ) * 2.0;
  const float YtoZ = 0.83359; // tan( 0.7898090 / 2.0 ) * 2.0;

  void main() {

    vUv = vec2( position.x / width, position.y / height );

    vec4 color = texture2D( map, vUv );
    float depth = ( color.r + color.g + color.b ) / 3.0;

    // Projection code by @kcmic

    float z = ( 1.0 - depth ) * (farClipping - nearClipping) + nearClipping;

    vec4 pos = vec4(
      ( position.x / width - 0.5 ) * z * XtoZ,
      ( position.y / height - 0.5 ) * z * YtoZ,
      - z + zOffset,
      1.0);

    gl_PointSize = pointSize;
    gl_Position = projectionMatrix * modelViewMatrix * pos;

  }

</script>

<script id="fs" type="x-shader/x-fragment">

  uniform sampler2D map;

  varying vec2 vUv;

  void main() {

    vec4 color = texture2D( map, vUv );
    gl_FragColor = vec4( color.r, color.g, color.b, 0.2 );

  }

</script>

<script>
//Setup three.js WebGL renderer
var renderer = new THREE.WebGLRenderer({ antialias: true });
renderer.setPixelRatio(window.devicePixelRatio);

// Append the canvas element created by the renderer to document body element.
document.body.appendChild(renderer.domElement);

// Create a three.js scene.
var scene = new THREE.Scene();

// Create a three.js camera.
var camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.3, 10000);

// Apply VR headset positional data to camera.
var controls = new THREE.VRControls(camera);

// Apply VR stereo rendering to renderer.
var effect = new THREE.VREffect(renderer);
effect.setSize(window.innerWidth, window.innerHeight);

// Create a VR manager helper to enter and exit VR mode.
var manager = new WebVRManager(renderer, effect, {hideButton: false});

var video = document.createElement( 'video' );
video.addEventListener( 'loadedmetadata', function ( event ) {

  var texture = new THREE.VideoTexture( video );
  texture.minFilter = THREE.NearestFilter;

  var width = 640, height = 480;
  var nearClipping = 850, farClipping = 4000;

  geometry = new THREE.BufferGeometry();

  var vertices = new Float32Array( width * height * 3 );

  for ( var i = 0, j = 0, l = vertices.length; i < l; i += 3, j ++ ) {

    vertices[ i ] = j % width;
    vertices[ i + 1 ] = Math.floor( j / width );

  }

  geometry.addAttribute( 'position', new THREE.BufferAttribute( vertices, 3 ) );

  material = new THREE.ShaderMaterial( {

    uniforms: {

      "map": { type: "t", value: texture },
      "width": { type: "f", value: width },
      "height": { type: "f", value: height },
      "nearClipping": { type: "f", value: nearClipping },
      "farClipping": { type: "f", value: farClipping },

      "pointSize": { type: "f", value: 2 },
      "zOffset": { type: "f", value: 1000 }

    },
    vertexShader: document.getElementById( 'vs' ).textContent,
    fragmentShader: document.getElementById( 'fs' ).textContent,
    blending: THREE.AdditiveBlending,
    depthTest: false, depthWrite: false,
    transparent: true

  } );

  mesh = new THREE.Points( geometry, material );
  scene.add( mesh );
},  false);

video.loop = true;
video.src = 'videos_src/kinect.webm';
video.play();

// Request animation frame loop function
function animate(timestamp) {
  // Apply rotation to cube mesh

  // Update VR headset position and apply to camera.
  controls.update();

  // Render the scene through the manager.
  manager.render(scene, camera, timestamp);

  requestAnimationFrame(animate);
}

// Kick off animation loop
animate();

// Reset the position sensor when 'z' pressed.
function onKey(event) {
  if (event.keyCode == 90) { // z
    controls.resetSensor();
  }
};

window.addEventListener('keydown', onKey, true);

// Handle window resizes
function onWindowResize() {
  camera.aspect = window.innerWidth / window.innerHeight;
  camera.updateProjectionMatrix();

  effect.setSize(window.innerWidth, window.innerHeight);
}

window.addEventListener('resize', onWindowResize, false);

</script>

</html>
